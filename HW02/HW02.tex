\documentclass[11pt]{article}
\usepackage{euscript}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{xspace}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{url}
\usepackage{bbm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\textheight}{9in}
\setlength{\topmargin}{-0.600in}
\setlength{\headheight}{0.2in}
\setlength{\headsep}{0.250in}
\setlength{\footskip}{0.5in}
\flushbottom
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\columnsep}{2pc}
\setlength{\parindent}{1em}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{epsfig}
\usepackage{xspace}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{url}
%%%%%%%  For drawing trees  %%%%%%%%%
\usepackage{tikz}
\usetikzlibrary{calc, shapes, backgrounds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\textheight}{9in}
\setlength{\topmargin}{-0.600in}
\setlength{\headheight}{0.2in}
\setlength{\headsep}{0.250in}
\setlength{\footskip}{0.5in}
\flushbottom
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\columnsep}{2pc}
\setlength{\parindent}{1em}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\eps}{\varepsilon}
\renewcommand{\c}[1]{\ensuremath{\EuScript{#1}}}
\renewcommand{\b}[1]{\ensuremath{\mathbb{#1}}}
\renewcommand{\theenumi}{\alph{enumi}}
\newcommand{\s}[1]{\textsf{#1}}
\newcommand{\E}{\textbf{\textsf{E}}}
\renewcommand{\Pr}{\textbf{\textsf{Pr}}}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\vspace{.5em}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand\algorithm{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\pagestyle{fancyplain}

\graphicspath{ {.} }

\lhead{\textbf{\NAME\ (\UNI)}}
\chead{\textbf{HW\HWNUM}}
\rhead{COMS W4721, \today}


\newcommand\NAME{Daniel Kronovet}  
\newcommand\UNI{003349897}    
\newcommand\HWNUM{02}          

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Problem 1 (multiclass logistic regression)}

Part 1:
\begin{enumerate}
\item Write out the log likelihood $L$ of data $(x_1, y_1), . . . , (x_n, y_n)$ using an i.i.d. assumption.

First, we observe that while $p(y|x, w_1, ..., w_n)$ is a product of the likelihood of each $w$, the indicator effectively cancels out every $w_i, i \neq c$ as $z^{\mathbbm{1}(y = c)} = z^0 = 1$, where $c$ is the .

With that in mind, we can cast the joint probability of the data as:

\[
L = \prod_{i=1}^{n} (\frac{e^{x_i^Tw_i}}{\sum_{j=1}^k e^{x_i^Tw_j}})^{\mathbbm{1}(y = i)}
\]

Taking the natural log of this, we have:

\[
lnL = \sum_{i=1}^{n} \mathbbm{1}(y = i) ln(\frac{e^{x_i^Tw_i}}{\sum_{j=1}^k e^{x_i^Tw_j}})
\]

\[
lnL = \sum_{i=1}^{n} \mathbbm{1}(y = i) (x_i^Tw_i - ln{\sum_j^k e^{x_i^Tw_j}})
\]

\item Calculate $\nabla_{w_i} L$ and $\nabla^2_{w_i} L$.

Beginning with the log likelihood:

\[
lnL = \sum_{i=1}^{n} \mathbbm{1}(y = i) (x_i^Tw_i - ln{\sum_j^k e^{x_i^Tw_j}})
\]

\[
\nabla_{w_i}lnL = \sum_{i=1}^{n} \mathbbm{1}(y = i) (x_i - \frac{1}{\sum_j^k e^{x_i^Tw_j}} (e^{x_i^Tw_i}) x_i)
\]

\[
\nabla_{w_i}lnL = \sum_{i=1}^{n} \mathbbm{1}(y = i)(x_i) (1 - \frac{e^{x_i^Tw_i}}{\sum_j^k e^{x_i^Tw_j}})
\]
ALT:
\[
\nabla_{w_i}lnL = \sum_{i=1}^{n}(x_i) (1 - \frac{e^{x_i^Tw_i}}{\sum_j^k e^{x_i^Tw_j}})
\]

The numerator $e^{x_i^Tw_i}$ makes sense because we take the gradient with respect to $w_i$. Therefore, the terms of the summation are 0 for every $w_j, w_j \neq w_i$.
To interpret the gradient, we see how the greater the probability of $x_i$, the smaller $\nabla_{w_i}lnL$, or the less of an impact a change in $w_i$ will have on the likelihood.


\end{enumerate}

\section*{Problem 2 (Gaussian kernels)}

\begin{enumerate}
\item Use Bayes rule to derive the posterior distribution of $\lambda$ and identify the name of this distribution.

By Bayes rule, we know that:
\[
P(\lambda | x) = \frac{P(x | \lambda) P(\lambda)}{P(x)}
\]

Given that we are interested in $\lambda$, we can discard the denominator (which ultimately does not involve $\lambda$) and rewrite this equation as:

\[
P(\lambda | x) \propto P(x | \lambda) P(\lambda)
\]

We can then expand the right side of the equation into the joint distribution of $P(x, \lambda)$:

\[
P(\lambda | x) \propto  (\prod_{i=1}^{n} \frac{\lambda^{x_i}}{x!} e^{-\lambda}) \frac{\beta_1^{\alpha_1}}{\Gamma(\alpha_1)} \lambda^{\alpha_1 - 1} e^{-\beta_1 \lambda}
\]
\[
\propto  (\frac{\lambda^{\sum x_i}}{\prod x!} e^{-n\lambda}) \frac{\beta_1^{\alpha_1}}{\Gamma(\alpha_1)} \lambda^{\alpha_1 - 1} e^{-\beta_1 \lambda}
\]
\[
\propto  \frac{\beta_1^{\alpha_1}}{\Gamma(\alpha_1) \prod x!} \lambda^{(\sum x_i + \alpha_1) - 1} e^{-\lambda (n + \beta_1)})
\]

From this we have enough to learn the parameters of the posterior Gamma distribution: $\alpha_2 = \sum x_i + \alpha_1$ and $\beta_2 = n + \beta_1$. This result is still not the complete distribution, however: the fraction is not in the correct form for a Gamma, nor have we addressed the denominator which we dropped earlier. We know, however, that the final distribution must integrate to 1, which allows us to derive the correct values for the other terms (although we will not do so here).

\item What is the mean and variance of $\lambda$ under this posterior? Discuss how this relates to your solution to Part 2 of Problem 1.

The posterior $\lambda$ is $~Gamma(\sum x_i + \alpha_1, n + \beta_1)$

As such, the mean of $\lambda = \frac{\alpha_2}{\beta_2} = \frac{\sum x_i + \alpha_1}{n + \beta_1}$.

The variance is $\frac{\alpha_2}{\beta_2^2} = \frac{\sum x_i + \alpha_1}{(n + \beta_1)^2}$.

In 1.2, we saw that: $\frac{\sum x_i}{n} = \hat \lambda_{ML}$.

There is a clear parallel to the Bayesian posterior mean: $\frac{\sum x_i + \alpha_1}{n + \beta_1} = E(\lambda)$, with the difference being the influence of the prior parameters $\alpha_1$ and $\beta_1$. This relationship represents the Bayesian insight by which our posterior distribution is determined by an interaction between our prior beliefs ($\alpha_1, \beta_1$) and our data ($\sum x_i, n$).

\end{enumerate}



\section*{Problem 3 (Classification)}

Part 1

\begin{table}[!th]
\centering
\begin{tabular}{|l|r|}
\hline
K &  Acc \\
\hline
1 & .948 \\
2 & .93  \\
3 & .918 \\
4 & .902 \\
5 & .894 \\
\hline
\end{tabular}
\caption{Prediction accuracy for KNN}
\label{ex:table}
\end{table}


Part 2
\begin{enumerate}
\item In a table, print the mean and standard deviation of the RMSE as a function of $p$. Using these numbers argue for which value of $p$ is the best.

As this table shows, setting $p$ = 3 gives both the lowest RMSE and the lowest variance for any value of $p$.

\begin{table}[!th]
\centering
\begin{tabular}{|l|c|r|}
\hline
$p$ & Mean & Std \\
\hline
1 & 3.44030468834 & 0.672995051589 \\
2 & 2.75919186824 & 0.623061482354 \\
3 & 2.64085582004 & 0.602524538829 \\
4 & 2.73039848277 & 0.697498334382 \\
\hline
\end{tabular}
\caption{RMSE by $p$}
\label{ex:table}
\end{table}

\item For each value of $p$, collect $y_{test} - y_{pred}$ for each test example. (Observe that this number can be negative, and there are 20 x 1000 in total.) Plot a histogram of these errors for each $p$.



\item For each $p$, use maximum likelihood to fit a univariate Gaussian to the 20,000 errors from Part 2(b). Describe how you calculated the maximum likelihood values for the mean and variance (this is a univariate case of what we did in class, so no need to re-derive it). What is the log likelihood of these empirical errors using the maximum likelihood values for the mean and variance? Show this as a function of $p$ and discuss how this agrees/disagrees with your conclusion in Part 2(a). What assumptions are best satisfied by the optimal value of $p$ using this approach?

To fit the Gaussian, we calculated both $\hat \mu_{ML}$ and $\hat \sigma^2_{ML}$ using the standard maximum-likelihood estimators for these parameters: $\frac{1}{n} \sum_{i=1}^{n} x_i$ and $\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \hat \mu_{ML})^2$.

Looking at this table, $p = 3$ again appears to create the strongest model. The variance (although not the mean) of the error is lowest with this model. Additionally, the log likelihood of this error is the greatest, although by a small margin, compared to all other models. This supports our conclusions in 2(a), where we identified $p = 3$ as being the strongest model.

\begin{table}[!th]
\centering
\begin{tabular}{| l| c | c | r|}
\hline
$p$ & Mean & Var & Log Likelihood \\
\hline
1 & 0.0163194355852 & 3.50549766331 & -53464.9206058 \\
2 & -0.0285981551706 & 2.82852246649 & -49173.3602374 \\
3 & 0.0409854631276 & 2.70840898169 & -48305.4980812 \\
4 & 0.0593496182852 & 2.81743994052 & -49094.8436665 \\
\hline
\end{tabular}
\caption{Log Likelihood of error by $p$}
\label{ex:table}
\end{table}

\end{enumerate}


\end{document}
